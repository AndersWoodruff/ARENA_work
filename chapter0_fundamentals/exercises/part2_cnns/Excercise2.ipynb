{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "from IPython.display import display\n",
    "from jaxtyping import Float, Int, jaxtyped\n",
    "from typeguard import typechecked\n",
    "from PIL import Image\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, models, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = \"chapter0_fundamentals\"\n",
    "section = \"part2_cnns\"\n",
    "root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())\n",
    "exercises_dir = root_dir / chapter / \"exercises\"\n",
    "section_dir = exercises_dir / section\n",
    "if str(exercises_dir) not in sys.path:\n",
    "    sys.path.append(str(exercises_dir))\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "\n",
    "import part2_cnns.tests as tests\n",
    "import part2_cnns.utils as utils\n",
    "from plotly_utils import line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_relu` passed!\n"
     ]
    }
   ],
   "source": [
    "class ReLU(nn.Module):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return t.maximum(x, t.zeros_like(x))\n",
    "\n",
    "\n",
    "tests.test_relu(ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_linear_parameters` passed!\n",
      "All tests in `test_linear_parameters` passed!\n",
      "All tests in `test_linear_forward` passed!\n",
      "All tests in `test_linear_forward` passed!\n"
     ]
    }
   ],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias=True):\n",
    "        \"\"\"\n",
    "        A simple linear (technically, affine) transformation.\n",
    "\n",
    "        The fields should be named `weight` and `bias` for compatibility with PyTorch.\n",
    "        If `bias` is False, set `self.bias` to None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        weight = (t.rand((out_features, in_features)) - 0.5) * 2/t.sqrt(t.tensor(in_features))\n",
    "        self.weight = nn.Parameter(weight)\n",
    "\n",
    "        if bias:\n",
    "            bias = (t.rand((out_features))  - 0.5) * 2/t.sqrt(t.tensor(in_features))\n",
    "            self.bias = nn.Parameter(bias)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        x: shape (*, in_features)\n",
    "        Return: shape (*, out_features)\n",
    "        \"\"\"\n",
    "        if self.bias is not None:\n",
    "            return t.matmul(x, t.transpose(self.weight, 0, 1)) + self.bias\n",
    "        else:\n",
    "            return t.matmul(x, t.transpose(self.weight, 0, 1))\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return \"-\"*15+f\"in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}\"\n",
    "\n",
    "\n",
    "tests.test_linear_parameters(Linear, bias=False)\n",
    "tests.test_linear_parameters(Linear, bias=True)\n",
    "tests.test_linear_forward(Linear, bias=False)\n",
    "tests.test_linear_forward(Linear, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self, start_dim: int = 1, end_dim: int = -1) -> None:\n",
    "        super().__init__()\n",
    "        self.start_dim = start_dim\n",
    "        self.end_dim = end_dim\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Flatten out dimensions from start_dim to end_dim, inclusive of both.\n",
    "        \"\"\"\n",
    "        shape = input.shape\n",
    "\n",
    "        # Get start & end dims, handling negative indexing for end dim\n",
    "        start_dim = self.start_dim\n",
    "        end_dim = self.end_dim if self.end_dim >= 0 else len(shape) + self.end_dim\n",
    "\n",
    "        # Get the shapes to the left / right of flattened dims, as well as the size of the flattened middle\n",
    "        shape_left = shape[:start_dim]\n",
    "        shape_right = shape[end_dim + 1 :]\n",
    "        shape_middle = t.prod(t.tensor(shape[start_dim : end_dim + 1])).item()\n",
    "\n",
    "        return t.reshape(input, shape_left + (shape_middle,) + shape_right)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return \", \".join([f\"{key}={getattr(self, key)}\" for key in [\"start_dim\", \"end_dim\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_mlp_module` passed!\n",
      "All tests in `test_mlp_forward` passed!\n"
     ]
    }
   ],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = Flatten()\n",
    "        self.relu = ReLU()\n",
    "        self.linear1 = Linear(28**2, 100)\n",
    "        self.linear2 = Linear(100, 10)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "tests.test_mlp_module(SimpleMLP)\n",
    "tests.test_mlp_forward(SimpleMLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Training Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = t.device(\"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_batch.shape=torch.Size([64, 1, 28, 28])\n",
      "label_batch.shape=torch.Size([64])\n",
      "\n",
      "img.shape=torch.Size([1, 28, 28])\n",
      "label=7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MNIST_TRANSFORM = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0.1307, 0.3081),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def get_mnist(trainset_size: int = 10_000, testset_size: int = 1_000) -> tuple[Subset, Subset]:\n",
    "    \"\"\"Returns a subset of MNIST training data.\"\"\"\n",
    "\n",
    "    # Get original datasets, which are downloaded to \"chapter0_fundamentals/exercises/data\" for future use\n",
    "    mnist_trainset = datasets.MNIST(exercises_dir / \"data\", train=True, download=True, transform=MNIST_TRANSFORM)\n",
    "    mnist_testset = datasets.MNIST(exercises_dir / \"data\", train=False, download=True, transform=MNIST_TRANSFORM)\n",
    "\n",
    "    # # Return a subset of the original datasets\n",
    "    mnist_trainset = Subset(mnist_trainset, indices=range(trainset_size))\n",
    "    mnist_testset = Subset(mnist_testset, indices=range(testset_size))\n",
    "\n",
    "    return mnist_trainset, mnist_testset\n",
    "\n",
    "\n",
    "mnist_trainset, mnist_testset = get_mnist()\n",
    "mnist_trainloader = DataLoader(mnist_trainset, batch_size=64, shuffle=True)\n",
    "mnist_testloader = DataLoader(mnist_testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Get the first batch of test data, by starting to iterate over `mnist_testloader`\n",
    "for img_batch, label_batch in mnist_testloader:\n",
    "    print(f\"{img_batch.shape=}\\n{label_batch.shape=}\\n\")\n",
    "    break\n",
    "\n",
    "# Get the first datapoint in the test set, by starting to iterate over `mnist_testset`\n",
    "for img, label in mnist_testset:\n",
    "    print(f\"{img.shape=}\\n{label=}\\n\")\n",
    "    break\n",
    "\n",
    "t.testing.assert_close(img, img_batch[0])\n",
    "assert label == label_batch[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905773bc41194752a1683d58c8a11646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80605434b054d3d80d6016d8356d38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a75b9b975dc4572a953f92e1c0edbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d2ec4fd22b44d3b716f5e64c434a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95866734d0b49cd9b6e4e5464bf4ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd0d38c09f14e009a1fa947165c8925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "displaylogo": false,
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Cross entropy loss",
         "type": "scatter",
         "x": [
          0,
          63.829787234042556,
          127.65957446808511,
          191.48936170212767,
          255.31914893617022,
          319.1489361702128,
          382.97872340425533,
          446.8085106382979,
          510.63829787234044,
          574.468085106383,
          638.2978723404256,
          702.1276595744681,
          765.9574468085107,
          829.7872340425532,
          893.6170212765958,
          957.4468085106383,
          1021.2765957446809,
          1085.1063829787236,
          1148.936170212766,
          1212.7659574468084,
          1276.595744680851,
          1340.4255319148938,
          1404.2553191489362,
          1468.0851063829787,
          1531.9148936170213,
          1595.744680851064,
          1659.5744680851064,
          1723.404255319149,
          1787.2340425531916,
          1851.0638297872342,
          1914.8936170212767,
          1978.723404255319,
          2042.5531914893618,
          2106.3829787234044,
          2170.212765957447,
          2234.0425531914893,
          2297.872340425532,
          2361.7021276595747,
          2425.531914893617,
          2489.3617021276596,
          2553.191489361702,
          2617.021276595745,
          2680.8510638297876,
          2744.68085106383,
          2808.5106382978724,
          2872.340425531915,
          2936.1702127659573,
          3000,
          3063.8297872340427,
          3127.6595744680853,
          3191.489361702128,
          3255.31914893617,
          3319.148936170213,
          3382.9787234042556,
          3446.808510638298,
          3510.6382978723404,
          3574.468085106383,
          3638.297872340426,
          3702.1276595744685,
          3765.9574468085107,
          3829.7872340425533,
          3893.617021276596,
          3957.446808510638,
          4021.276595744681,
          4085.1063829787236,
          4148.936170212766,
          4212.765957446809,
          4276.595744680852,
          4340.425531914894,
          4404.255319148936,
          4468.085106382979,
          4531.914893617021,
          4595.744680851064,
          4659.574468085107,
          4723.404255319149,
          4787.234042553192,
          4851.063829787234,
          4914.893617021276,
          4978.723404255319,
          5042.553191489362,
          5106.382978723404,
          5170.212765957447,
          5234.04255319149,
          5297.8723404255325,
          5361.702127659575,
          5425.531914893617,
          5489.36170212766,
          5553.191489361702,
          5617.021276595745,
          5680.851063829788,
          5744.68085106383,
          5808.510638297873,
          5872.340425531915,
          5936.170212765957,
          6000,
          6063.829787234043,
          6127.659574468085,
          6191.489361702128,
          6255.319148936171,
          6319.148936170213,
          6382.978723404256,
          6446.808510638298,
          6510.63829787234,
          6574.468085106383,
          6638.297872340426,
          6702.127659574468,
          6765.957446808511,
          6829.787234042554,
          6893.617021276596,
          6957.446808510638,
          7021.276595744681,
          7085.106382978724,
          7148.936170212766,
          7212.765957446809,
          7276.595744680852,
          7340.425531914894,
          7404.255319148937,
          7468.085106382979,
          7531.914893617021,
          7595.744680851064,
          7659.574468085107,
          7723.404255319149,
          7787.234042553192,
          7851.063829787235,
          7914.893617021276,
          7978.723404255319,
          8042.553191489362,
          8106.382978723404,
          8170.212765957447,
          8234.04255319149,
          8297.872340425532,
          8361.702127659575,
          8425.531914893618,
          8489.36170212766,
          8553.191489361703,
          8617.021276595746,
          8680.851063829788,
          8744.68085106383,
          8808.510638297872,
          8872.340425531915,
          8936.170212765957,
          9000,
          9063.829787234043,
          9127.659574468085,
          9191.489361702128,
          9255.31914893617,
          9319.148936170213,
          9382.978723404256,
          9446.808510638299,
          9510.638297872341,
          9574.468085106384,
          9638.297872340427,
          9702.127659574468,
          9765.95744680851,
          9829.787234042553,
          9893.617021276596,
          9957.446808510638,
          10021.27659574468,
          10085.106382978724,
          10148.936170212766,
          10212.765957446809,
          10276.595744680852,
          10340.425531914894,
          10404.255319148937,
          10468.08510638298,
          10531.914893617022,
          10595.744680851065,
          10659.574468085108,
          10723.40425531915,
          10787.234042553191,
          10851.063829787234,
          10914.893617021276,
          10978.72340425532,
          11042.553191489362,
          11106.382978723404,
          11170.212765957447,
          11234.04255319149,
          11297.872340425532,
          11361.702127659575,
          11425.531914893618,
          11489.36170212766,
          11553.191489361703,
          11617.021276595746,
          11680.851063829788,
          11744.68085106383,
          11808.510638297872,
          11872.340425531915,
          11936.170212765957,
          12000,
          12063.829787234043,
          12127.659574468085,
          12191.489361702128,
          12255.31914893617,
          12319.148936170213,
          12382.978723404256,
          12446.808510638299,
          12510.638297872341,
          12574.468085106384,
          12638.297872340427,
          12702.12765957447,
          12765.957446808512,
          12829.787234042553,
          12893.617021276596,
          12957.446808510638,
          13021.27659574468,
          13085.106382978724,
          13148.936170212766,
          13212.765957446809,
          13276.595744680852,
          13340.425531914894,
          13404.255319148937,
          13468.08510638298,
          13531.914893617022,
          13595.744680851065,
          13659.574468085108,
          13723.40425531915,
          13787.234042553191,
          13851.063829787234,
          13914.893617021276,
          13978.72340425532,
          14042.553191489362,
          14106.382978723404,
          14170.212765957447,
          14234.04255319149,
          14297.872340425532,
          14361.702127659575,
          14425.531914893618,
          14489.36170212766,
          14553.191489361703,
          14617.021276595746,
          14680.851063829788,
          14744.680851063831,
          14808.510638297874,
          14872.340425531915,
          14936.170212765957,
          15000,
          15063.829787234043,
          15127.659574468085,
          15191.489361702128,
          15255.31914893617,
          15319.148936170213,
          15382.978723404256,
          15446.808510638299,
          15510.638297872341,
          15574.468085106384,
          15638.297872340427,
          15702.12765957447,
          15765.957446808512,
          15829.787234042553,
          15893.617021276596,
          15957.446808510638,
          16021.27659574468,
          16085.106382978724,
          16148.936170212766,
          16212.765957446809,
          16276.595744680852,
          16340.425531914894,
          16404.255319148935,
          16468.08510638298,
          16531.91489361702,
          16595.744680851065,
          16659.574468085106,
          16723.40425531915,
          16787.23404255319,
          16851.063829787236,
          16914.893617021276,
          16978.72340425532,
          17042.55319148936,
          17106.382978723406,
          17170.212765957447,
          17234.04255319149,
          17297.872340425532,
          17361.702127659577,
          17425.531914893618,
          17489.36170212766,
          17553.191489361703,
          17617.021276595744,
          17680.85106382979,
          17744.68085106383,
          17808.510638297874,
          17872.340425531915,
          17936.17021276596,
          18000,
          18063.829787234044,
          18127.659574468085,
          18191.48936170213,
          18255.31914893617,
          18319.148936170215,
          18382.978723404256,
          18446.808510638297,
          18510.63829787234,
          18574.468085106382,
          18638.297872340427,
          18702.127659574468,
          18765.957446808512,
          18829.787234042553,
          18893.617021276597,
          18957.44680851064,
          19021.276595744683,
          19085.106382978724,
          19148.936170212768,
          19212.76595744681,
          19276.595744680853,
          19340.425531914894,
          19404.255319148935,
          19468.08510638298,
          19531.91489361702,
          19595.744680851065,
          19659.574468085106,
          19723.40425531915,
          19787.23404255319,
          19851.063829787236,
          19914.893617021276,
          19978.72340425532,
          20042.55319148936,
          20106.382978723406,
          20170.212765957447,
          20234.04255319149,
          20297.872340425532,
          20361.702127659577,
          20425.531914893618,
          20489.36170212766,
          20553.191489361703,
          20617.021276595744,
          20680.85106382979,
          20744.68085106383,
          20808.510638297874,
          20872.340425531915,
          20936.17021276596,
          21000,
          21063.829787234044,
          21127.659574468085,
          21191.48936170213,
          21255.31914893617,
          21319.148936170215,
          21382.978723404256,
          21446.8085106383,
          21510.63829787234,
          21574.468085106382,
          21638.297872340427,
          21702.127659574468,
          21765.957446808512,
          21829.787234042553,
          21893.617021276597,
          21957.44680851064,
          22021.276595744683,
          22085.106382978724,
          22148.936170212768,
          22212.76595744681,
          22276.595744680853,
          22340.425531914894,
          22404.25531914894,
          22468.08510638298,
          22531.91489361702,
          22595.744680851065,
          22659.574468085106,
          22723.40425531915,
          22787.23404255319,
          22851.063829787236,
          22914.893617021276,
          22978.72340425532,
          23042.55319148936,
          23106.382978723406,
          23170.212765957447,
          23234.04255319149,
          23297.872340425532,
          23361.702127659577,
          23425.531914893618,
          23489.36170212766,
          23553.191489361703,
          23617.021276595744,
          23680.85106382979,
          23744.68085106383,
          23808.510638297874,
          23872.340425531915,
          23936.17021276596,
          24000,
          24063.829787234044,
          24127.659574468085,
          24191.48936170213,
          24255.31914893617,
          24319.148936170215,
          24382.978723404256,
          24446.8085106383,
          24510.63829787234,
          24574.468085106382,
          24638.297872340427,
          24702.127659574468,
          24765.957446808512,
          24829.787234042553,
          24893.617021276597,
          24957.44680851064,
          25021.276595744683,
          25085.106382978724,
          25148.936170212768,
          25212.76595744681,
          25276.595744680853,
          25340.425531914894,
          25404.25531914894,
          25468.08510638298,
          25531.914893617024,
          25595.744680851065,
          25659.574468085106,
          25723.40425531915,
          25787.23404255319,
          25851.063829787236,
          25914.893617021276,
          25978.72340425532,
          26042.55319148936,
          26106.382978723406,
          26170.212765957447,
          26234.04255319149,
          26297.872340425532,
          26361.702127659577,
          26425.531914893618,
          26489.361702127662,
          26553.191489361703,
          26617.021276595744,
          26680.85106382979,
          26744.68085106383,
          26808.510638297874,
          26872.340425531915,
          26936.17021276596,
          27000,
          27063.829787234044,
          27127.659574468085,
          27191.48936170213,
          27255.31914893617,
          27319.148936170215,
          27382.978723404256,
          27446.8085106383,
          27510.63829787234,
          27574.468085106382,
          27638.297872340427,
          27702.127659574468,
          27765.957446808512,
          27829.787234042553,
          27893.617021276597,
          27957.44680851064,
          28021.276595744683,
          28085.106382978724,
          28148.936170212768,
          28212.76595744681,
          28276.595744680853,
          28340.425531914894,
          28404.25531914894,
          28468.08510638298,
          28531.914893617024,
          28595.744680851065,
          28659.574468085106,
          28723.40425531915,
          28787.23404255319,
          28851.063829787236,
          28914.893617021276,
          28978.72340425532,
          29042.55319148936,
          29106.382978723406,
          29170.212765957447,
          29234.04255319149,
          29297.872340425532,
          29361.702127659577,
          29425.531914893618,
          29489.361702127662,
          29553.191489361703,
          29617.021276595748,
          29680.85106382979,
          29744.68085106383,
          29808.510638297874,
          29872.340425531915,
          29936.17021276596,
          30000
         ],
         "xaxis": "x",
         "y": [
          2.289816379547119,
          2.1807894706726074,
          2.0664806365966797,
          1.9008675813674927,
          1.88278329372406,
          1.7486659288406372,
          1.639470100402832,
          1.3627201318740845,
          1.499657154083252,
          1.32965087890625,
          1.1349269151687622,
          1.1213886737823486,
          1.0976775884628296,
          1.1624109745025635,
          1.1392552852630615,
          1.0042227506637573,
          1.0803011655807495,
          0.8250808715820312,
          0.7474339008331299,
          0.8377876877784729,
          0.8197050094604492,
          0.7475678324699402,
          0.740494430065155,
          0.5673348903656006,
          0.4953765869140625,
          0.6965190768241882,
          0.8526453375816345,
          0.6667530536651611,
          0.5246647000312805,
          0.48877814412117004,
          0.5290709733963013,
          0.4819704294204712,
          0.5796940922737122,
          0.5177311897277832,
          0.586801290512085,
          0.3906775712966919,
          0.6060642600059509,
          0.4794696569442749,
          0.5604870915412903,
          0.5050845146179199,
          0.5905216336250305,
          0.3770751953125,
          0.41752320528030396,
          0.546006977558136,
          0.46981897950172424,
          0.5150851607322693,
          0.32109498977661133,
          0.580355167388916,
          0.2876488268375397,
          0.42836764454841614,
          0.38026684522628784,
          0.4023062288761139,
          0.26408302783966064,
          0.31484368443489075,
          0.2639636993408203,
          0.27794599533081055,
          0.2841041684150696,
          0.4428587853908539,
          0.4873254895210266,
          0.5397421717643738,
          0.4057233929634094,
          0.3540118336677551,
          0.3416268527507782,
          0.4112316966056824,
          0.30836203694343567,
          0.3389489948749542,
          0.331288605928421,
          0.3124944865703583,
          0.5343214273452759,
          0.34945443272590637,
          0.4910060167312622,
          0.5743511319160461,
          0.30369389057159424,
          0.4583882987499237,
          0.3497360348701477,
          0.3598243296146393,
          0.30166664719581604,
          0.35650521516799927,
          0.35554468631744385,
          0.1972309947013855,
          0.5232561230659485,
          0.222740039229393,
          0.39342570304870605,
          0.3443954885005951,
          0.39462846517562866,
          0.28967782855033875,
          0.26662296056747437,
          0.45321935415267944,
          0.2788272202014923,
          0.2773362994194031,
          0.4353272318840027,
          0.2227681279182434,
          0.1745568960905075,
          0.25225284695625305,
          0.5000807642936707,
          0.24970702826976776,
          0.31373852491378784,
          0.4920172095298767,
          0.4873765707015991,
          0.4333864450454712,
          0.23496557772159576,
          0.2210540771484375,
          0.41227248311042786,
          0.3627719581127167,
          0.41404885053634644,
          0.605023205280304,
          0.3234497606754303,
          0.32334429025650024,
          0.7150751352310181,
          0.32932814955711365,
          0.7103797793388367,
          0.566230297088623,
          0.5336496233940125,
          0.4476379156112671,
          0.2956472933292389,
          0.1815597414970398,
          0.398891419172287,
          0.466195672750473,
          0.3049132227897644,
          0.3683093786239624,
          0.5058479309082031,
          0.23421356081962585,
          0.25417909026145935,
          0.6866190433502197,
          0.19436302781105042,
          0.31930020451545715,
          0.31261464953422546,
          0.4032914340496063,
          0.27628615498542786,
          0.3035004734992981,
          0.29407984018325806,
          0.2644837498664856,
          0.471219003200531,
          0.481438547372818,
          0.3649377226829529,
          0.275680273771286,
          0.26215094327926636,
          0.5494486093521118,
          0.20334948599338531,
          0.2725810408592224,
          0.3734860122203827,
          0.27836310863494873,
          0.28805217146873474,
          0.2817833423614502,
          0.3223413825035095,
          0.2709745466709137,
          0.3087630271911621,
          0.43312662839889526,
          0.19447334110736847,
          0.2709617018699646,
          0.256032794713974,
          0.23518875241279602,
          0.4278905391693115,
          0.35225123167037964,
          0.11018075048923492,
          0.17342562973499298,
          0.2967371642589569,
          0.23862919211387634,
          0.41066116094589233,
          0.17479631304740906,
          0.4382375478744507,
          0.37116944789886475,
          0.2684411406517029,
          0.26165246963500977,
          0.20615635812282562,
          0.19047103822231293,
          0.19948147237300873,
          0.20762555301189423,
          0.35438990592956543,
          0.18215925991535187,
          0.31802529096603394,
          0.23915615677833557,
          0.22425024211406708,
          0.14260494709014893,
          0.26767510175704956,
          0.24573887884616852,
          0.2603176236152649,
          0.24354055523872375,
          0.4522504210472107,
          0.09279006719589233,
          0.2803632318973541,
          0.4648876488208771,
          0.22383220493793488,
          0.4342590570449829,
          0.23145197331905365,
          0.18976588547229767,
          0.2982128858566284,
          0.3029583990573883,
          0.27291005849838257,
          0.4654252529144287,
          0.28743550181388855,
          0.16991707682609558,
          0.324953556060791,
          0.2417360544204712,
          0.2263694554567337,
          0.28614693880081177,
          0.2532408535480499,
          0.30578169226646423,
          0.3580005168914795,
          0.44800207018852234,
          0.23888957500457764,
          0.2861475646495819,
          0.2768322825431824,
          0.3196300268173218,
          0.18388599157333374,
          0.15217216312885284,
          0.16018535196781158,
          0.36347639560699463,
          0.38766324520111084,
          0.26670223474502563,
          0.20738907158374786,
          0.4832744002342224,
          0.44362732768058777,
          0.24099743366241455,
          0.14430320262908936,
          0.22406041622161865,
          0.1716773957014084,
          0.17972902953624725,
          0.1982731819152832,
          0.13194043934345245,
          0.2539200484752655,
          0.13154509663581848,
          0.1304938644170761,
          0.2590036392211914,
          0.27845239639282227,
          0.1701299250125885,
          0.6091627478599548,
          0.4431776702404022,
          0.32594525814056396,
          0.255070298910141,
          0.24191126227378845,
          0.17306317389011383,
          0.14996686577796936,
          0.25414136052131653,
          0.2550792098045349,
          0.2326153963804245,
          0.18918685615062714,
          0.17654795944690704,
          0.37052279710769653,
          0.414615660905838,
          0.5109827518463135,
          0.3507641553878784,
          0.18579816818237305,
          0.34426677227020264,
          0.13791561126708984,
          0.1071312353014946,
          0.13114605844020844,
          0.23414503037929535,
          0.4190395772457123,
          0.32680296897888184,
          0.14052721858024597,
          0.3182794451713562,
          0.308371365070343,
          0.2434684783220291,
          0.39487558603286743,
          0.2891887426376343,
          0.20813307166099548,
          0.2551082968711853,
          0.38675934076309204,
          0.18223269283771515,
          0.37896043062210083,
          0.26180925965309143,
          0.27864718437194824,
          0.25277894735336304,
          0.1962948441505432,
          0.22422772645950317,
          0.3302638530731201,
          0.3602614998817444,
          0.22411440312862396,
          0.11718904227018356,
          0.12618473172187805,
          0.32432758808135986,
          0.29788681864738464,
          0.1530914455652237,
          0.2656604051589966,
          0.23701626062393188,
          0.08341628313064575,
          0.24168473482131958,
          0.11545515805482864,
          0.2586594223976135,
          0.1533062905073166,
          0.3760134279727936,
          0.34523487091064453,
          0.22857113182544708,
          0.14849327504634857,
          0.1498730629682541,
          0.3136325478553772,
          0.16756194829940796,
          0.22404815256595612,
          0.2284466028213501,
          0.19590029120445251,
          0.15559278428554535,
          0.20616483688354492,
          0.19913585484027863,
          0.19940951466560364,
          0.1856648474931717,
          0.21620173752307892,
          0.3277912735939026,
          0.22022050619125366,
          0.2206345647573471,
          0.15106803178787231,
          0.29588785767555237,
          0.1453733742237091,
          0.18616633117198944,
          0.35613396763801575,
          0.1966727077960968,
          0.17746619880199432,
          0.20133671164512634,
          0.19542038440704346,
          0.209603413939476,
          0.2498185634613037,
          0.20215538144111633,
          0.15277419984340668,
          0.35663461685180664,
          0.21613506972789764,
          0.237010195851326,
          0.10244058817625046,
          0.214971661567688,
          0.1760086566209793,
          0.21542052924633026,
          0.10306979715824127,
          0.3651929199695587,
          0.21892127394676208,
          0.15933503210544586,
          0.0977567732334137,
          0.38826078176498413,
          0.14872196316719055,
          0.08912868797779083,
          0.17225542664527893,
          0.3690090775489807,
          0.11833175271749496,
          0.22324174642562866,
          0.3400926887989044,
          0.19080308079719543,
          0.06636165082454681,
          0.1195826530456543,
          0.24705912172794342,
          0.2117312103509903,
          0.19297116994857788,
          0.1836288869380951,
          0.2827621102333069,
          0.12907108664512634,
          0.13574476540088654,
          0.1711788773536682,
          0.31561195850372314,
          0.13330036401748657,
          0.10683150589466095,
          0.1914178878068924,
          0.27043765783309937,
          0.2024419903755188,
          0.17928016185760498,
          0.08880497515201569,
          0.19202636182308197,
          0.22276848554611206,
          0.07895198464393616,
          0.22925260663032532,
          0.12986138463020325,
          0.16468310356140137,
          0.13629092276096344,
          0.2501235604286194,
          0.24977625906467438,
          0.1472606509923935,
          0.2662051320075989,
          0.2323678582906723,
          0.1719171702861786,
          0.18215128779411316,
          0.3199743628501892,
          0.23317430913448334,
          0.23260706663131714,
          0.37537243962287903,
          0.08776456862688065,
          0.3002927601337433,
          0.1383771002292633,
          0.1641048938035965,
          0.22152896225452423,
          0.17414051294326782,
          0.15198788046836853,
          0.19988813996315002,
          0.27443137764930725,
          0.13340246677398682,
          0.32859376072883606,
          0.2500697374343872,
          0.32724282145500183,
          0.18142257630825043,
          0.20208261907100677,
          0.12471689283847809,
          0.17127972841262817,
          0.20963945984840393,
          0.23308348655700684,
          0.1874343603849411,
          0.10284785181283951,
          0.15430927276611328,
          0.22412163019180298,
          0.27439290285110474,
          0.33969977498054504,
          0.11993015557527542,
          0.15496712923049927,
          0.20827201008796692,
          0.2566812336444855,
          0.12394832819700241,
          0.2260960191488266,
          0.1122879907488823,
          0.22211968898773193,
          0.15308894217014313,
          0.16435442864894867,
          0.20021262764930725,
          0.061232779175043106,
          0.20749132335186005,
          0.23760998249053955,
          0.06564497202634811,
          0.13755671679973602,
          0.23386675119400024,
          0.13867536187171936,
          0.21450160443782806,
          0.3412344455718994,
          0.2965884804725647,
          0.19401763379573822,
          0.16039763391017914,
          0.2327703833580017,
          0.1994786262512207,
          0.2901190221309662,
          0.14548969268798828,
          0.13021019101142883,
          0.21177200973033905,
          0.2231115996837616,
          0.1453651636838913,
          0.1542397290468216,
          0.15840846300125122,
          0.17953121662139893,
          0.166390523314476,
          0.3107181489467621,
          0.2967933118343353,
          0.2396845668554306,
          0.400321900844574,
          0.23457764089107513,
          0.20715516805648804,
          0.19620421528816223,
          0.36407724022865295,
          0.17195042967796326,
          0.14806364476680756,
          0.12650993466377258,
          0.19329632818698883,
          0.160257488489151,
          0.23367616534233093,
          0.1999693065881729,
          0.3744478225708008,
          0.0518481582403183,
          0.08756164461374283,
          0.21125833690166473,
          0.2544333338737488,
          0.23616841435432434,
          0.13430346548557281,
          0.07115330547094345,
          0.12456376105546951,
          0.19800347089767456,
          0.17525842785835266,
          0.15720349550247192,
          0.12276939302682877,
          0.14852437376976013,
          0.25082021951675415,
          0.1243148148059845,
          0.19949007034301758,
          0.07072482258081436,
          0.19707991182804108,
          0.22626590728759766,
          0.17603014409542084,
          0.14301320910453796,
          0.25630131363868713,
          0.13533300161361694,
          0.22469700872898102,
          0.06328053027391434
         ],
         "yaxis": "y"
        },
        {
         "name": "Test Accuracy",
         "type": "scatter",
         "x": [
          0,
          10000,
          20000,
          30000
         ],
         "xaxis": "x",
         "y": [
          0.1,
          0.89,
          0.9,
          0.916
         ],
         "yaxis": "y2"
        }
       ],
       "layout": {
        "hovermode": "x unified",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "SimpleMLP training on MNIST"
        },
        "width": 800,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.94
         ],
         "title": {
          "text": "Num examples seen"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Cross entropy loss"
         }
        },
        "yaxis2": {
         "anchor": "x",
         "overlaying": "y",
         "side": "right",
         "title": {
          "text": "Test Accuracy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dataclass\n",
    "class SimpleMLPTrainingArgs:\n",
    "    \"\"\"\n",
    "    Defining this class implicitly creates an __init__ method, which sets arguments as below, e.g. self.batch_size=64.\n",
    "    Any of these fields can also be overridden when you create an instance, e.g. SimpleMLPTrainingArgs(batch_size=128).\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size: int = 64\n",
    "    epochs: int = 3\n",
    "    learning_rate: float = 1e-3\n",
    "\n",
    "\n",
    "def train(args: SimpleMLPTrainingArgs) -> tuple[list[float], list[float], SimpleMLP]:\n",
    "    \"\"\"\n",
    "    Trains & returns the model, using training parameters from the `args` object. Returns the model, and loss list.\n",
    "    \"\"\"\n",
    "    model = SimpleMLP().to(device)\n",
    "\n",
    "    mnist_trainset, mnist_testset = get_mnist()\n",
    "    mnist_trainloader = DataLoader(mnist_trainset, batch_size=args.batch_size, shuffle=True)\n",
    "    mnist_testloader = DataLoader(mnist_testset, batch_size=args.batch_size)\n",
    "\n",
    "    optimizer = t.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        pbar = tqdm(mnist_trainloader)\n",
    "\n",
    "        for imgs, labels in pbar:\n",
    "            # Move data to device, perform forward pass\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            logits = model(imgs)\n",
    "\n",
    "            # Calculate loss, perform backward pass\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Update logs & progress bar\n",
    "            loss_list.append(loss.item())\n",
    "            pbar.set_postfix(epoch=f\"{epoch+1}/{args.epochs}\", loss=f\"{loss:.3f}\")\n",
    "        \n",
    "        \n",
    "        with t.inference_mode():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for imgs, labels in tqdm(mnist_testloader):\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                \n",
    "                \n",
    "                logits = model(imgs)\n",
    "\n",
    "                max = t.argmax(logits, 1)\n",
    "                agreement = t.eq(max, labels).type(t.float)\n",
    "                correct += t.count_nonzero(agreement).item()\n",
    "                total += agreement.shape[0]\n",
    "            \n",
    "            accuracy_list.append(correct/total)\n",
    "\n",
    "    return loss_list, accuracy_list, model\n",
    "\n",
    "\n",
    "args = SimpleMLPTrainingArgs()\n",
    "loss_list, accuracy_list, model = train(args)\n",
    "\n",
    "line(\n",
    "    y=[loss_list, [0.1] + accuracy_list],  # we start by assuming a uniform accuracy of 10%\n",
    "    use_secondary_yaxis=True,\n",
    "    x_max=args.epochs * len(mnist_trainset),\n",
    "    labels={\"x\": \"Num examples seen\", \"y1\": \"Cross entropy loss\", \"y2\": \"Test Accuracy\"},\n",
    "    title=\"SimpleMLP training on MNIST\",\n",
    "    width=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_conv2d_module` passed!\n",
      "Manually verify that this is an informative repr: Conv2d(in_channels=24, out_channels=12, kernel_size=3, stride=2, padding=1)\n"
     ]
    }
   ],
   "source": [
    "class Conv2d(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0):\n",
    "        \"\"\"\n",
    "        Same as torch.nn.Conv2d with bias=False.\n",
    "\n",
    "        Name your weight field `self.weight` for compatibility with the PyTorch version.\n",
    "\n",
    "        We assume kernel is square, with height = width = `kernel_size`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        Weight = (t.rand((out_channels, in_channels, kernel_size, kernel_size)) - 0.5) * 2/t.sqrt(t.tensor(in_channels * kernel_size * kernel_size))\n",
    "        self.weight = nn.Parameter(Weight)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Apply the functional conv2d, which you can import.\"\"\"\n",
    "        return t.nn.functional.conv2d(x, self.weight, stride=self.stride, padding=self.padding)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        keys = [\"in_channels\", \"out_channels\", \"kernel_size\", \"stride\", \"padding\"]\n",
    "        return \", \".join([f\"{key}={getattr(self, key)}\" for key in keys])\n",
    "\n",
    "\n",
    "tests.test_conv2d_module(Conv2d)\n",
    "m = Conv2d(in_channels=24, out_channels=12, kernel_size=3, stride=2, padding=1)\n",
    "print(f\"Manually verify that this is an informative repr: {m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2d(nn.Module):\n",
    "    def __init__(self, kernel_size: int, stride: int | None = None, padding: int = 1):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Call the functional version of maxpool2d.\"\"\"\n",
    "        return F.max_pool2d(x, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        \"\"\"Add additional information to the string representation of this class.\"\"\"\n",
    "        return \", \".join([f\"{key}={getattr(self, key)}\" for key in [\"kernel_size\", \"stride\", \"padding\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(nn.Module):\n",
    "    _modules: dict[str, nn.Module]\n",
    "\n",
    "    def __init__(self, *modules: nn.Module):\n",
    "        super().__init__()\n",
    "        for index, mod in enumerate(modules):\n",
    "            self._modules[str(index)] = mod\n",
    "\n",
    "    def __getitem__(self, index: int) -> nn.Module:\n",
    "        index %= len(self._modules)  # deal with negative indices\n",
    "        return self._modules[str(index)]\n",
    "\n",
    "    def __setitem__(self, index: int, module: nn.Module) -> None:\n",
    "        index %= len(self._modules)  # deal with negative indices\n",
    "        self._modules[str(index)] = module\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Chain each module together, with the output from one feeding into the next one.\"\"\"\n",
    "        for mod in self._modules.values():\n",
    "            x = mod(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_batchnorm2d_module` passed!\n",
      "All tests in `test_batchnorm2d_forward` passed!\n",
      "All tests in `test_batchnorm2d_running_mean` passed!\n"
     ]
    }
   ],
   "source": [
    "class BatchNorm2d(nn.Module):\n",
    "    # The type hints below aren't functional, they're just for documentation\n",
    "    running_mean: Float[Tensor, \"num_features\"]\n",
    "    running_var: Float[Tensor, \"num_features\"]\n",
    "    num_batches_tracked: Int[Tensor, \"\"]  # This is how we denote a scalar tensor\n",
    "\n",
    "    def __init__(self, num_features: int, eps=1e-05, momentum=0.1):\n",
    "        \"\"\"\n",
    "        Like nn.BatchNorm2d with track_running_stats=True and affine=True.\n",
    "\n",
    "        Name the learnable affine parameters `weight` and `bias` in that order.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.weight = nn.Parameter(t.ones(num_features))\n",
    "        self.bias = nn.Parameter(t.zeros(num_features))\n",
    "\n",
    "        self.register_buffer(\"running_mean\", t.zeros(num_features))\n",
    "        self.register_buffer(\"running_var\", t.ones(num_features))\n",
    "        self.register_buffer(\"num_batches_tracked\", t.tensor(0))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Normalize each channel.\n",
    "\n",
    "        Compute the variance using `torch.var(x, unbiased=False)`\n",
    "        Hint: you may also find it helpful to use the argument `keepdim`.\n",
    "\n",
    "        x: shape (batch, channels, height, width)\n",
    "        Return: shape (batch, channels, height, width)\n",
    "        \"\"\"\n",
    "\n",
    "        weight = einops.repeat(self.weight, \"a -> 1 a 1 1\")\n",
    "        bias = einops.repeat(self.bias, \"a -> 1 a 1 1\")\n",
    "\n",
    "        if self.training:\n",
    "            \n",
    "            var = t.var(x, [0,2,3], correction=False, keepdim=True)\n",
    "            mean = t.mean(x, [0,2,3], keepdim=True)\n",
    "\n",
    "            self.running_var = t.squeeze(var) * self.momentum + self.running_var * (1 - self.momentum)\n",
    "            self.running_mean = t.squeeze(mean) * self.momentum + self.running_mean * (1 - self.momentum)\n",
    "            self.num_batches_tracked += 1\n",
    "\n",
    "            x = x - mean\n",
    "            x = x / t.sqrt(var + self.eps)\n",
    "            x = x * weight\n",
    "            x = x + bias\n",
    "\n",
    "            return x\n",
    "\n",
    "        else:\n",
    "            x = x - einops.repeat(self.running_mean, \"c -> 1 c 1 1\")\n",
    "            x = x / t.sqrt(einops.repeat(self.running_var, \"c -> 1 c 1 1\") + self.eps)\n",
    "            x = x * weight\n",
    "            x = x + bias\n",
    "\n",
    "            return x\n",
    "\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"Bias of {self.bias.shape} and weight of {self.weight.shape}. Running var {self.running_var[0]} and mean {self.running_mean[0]}\"\n",
    "\n",
    "\n",
    "tests.test_batchnorm2d_module(BatchNorm2d)\n",
    "tests.test_batchnorm2d_forward(BatchNorm2d)\n",
    "tests.test_batchnorm2d_running_mean(BatchNorm2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_averagepool` passed!\n"
     ]
    }
   ],
   "source": [
    "class AveragePool(nn.Module):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        x: shape (batch, channels, height, width)\n",
    "        Return: shape (batch, channels)\n",
    "        \"\"\"\n",
    "        return x.mean([2,3])\n",
    "\n",
    "\n",
    "tests.test_averagepool(AveragePool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed all tests when first_stride=1\n",
      "Passed all tests when first_stride>1\n",
      "All tests in `test_residual_block` passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ander\\AppData\\Local\\Temp\\ipykernel_5736\\155897608.py:33: UserWarning:\n",
      "\n",
      "As of jaxtyping version 0.2.24, jaxtyping now prefers the syntax\n",
      "```\n",
      "from jaxtyping import jaxtyped\n",
      "# Use your favourite typechecker: usually one of the two lines below.\n",
      "from typeguard import typechecked as typechecker\n",
      "from beartype import beartype as typechecker\n",
      "\n",
      "@jaxtyped(typechecker=typechecker)\n",
      "def foo(...):\n",
      "```\n",
      "and the old double-decorator syntax\n",
      "```\n",
      "@jaxtyped\n",
      "@typechecker\n",
      "def foo(...):\n",
      "```\n",
      "should no longer be used. (It will continue to work as it did before, but the new approach will produce more readable error messages.)\n",
      "In particular note that `typechecker` must be passed via keyword argument; the following is not valid:\n",
      "```\n",
      "@jaxtyped(typechecker)\n",
      "def foo(...):\n",
      "```\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_feats: int, out_feats: int, first_stride=1):\n",
    "        \"\"\"\n",
    "        A single residual block with optional downsampling.\n",
    "\n",
    "        For compatibility with the pretrained model, declare the left side branch first using a `\n",
    "        `.\n",
    "\n",
    "        If first_stride is > 1, this means the optional (conv + bn) should be present on the right branch. Declare it second using another `Sequential`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        is_shape_preserving = (first_stride == 1) and (in_feats == out_feats)  # determines if right branch is identity\n",
    "\n",
    "        self.leftbranch = Sequential(\n",
    "            Conv2d(in_channels=in_feats, out_channels=out_feats, kernel_size=3, stride=first_stride, padding=1),\n",
    "            BatchNorm2d(num_features=out_feats),\n",
    "            ReLU(),\n",
    "            Conv2d(in_channels=out_feats, out_channels=out_feats, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(num_features=out_feats),\n",
    "            \n",
    "        )\n",
    "\n",
    "\n",
    "        if is_shape_preserving:\n",
    "            self.rightbranch = lambda x: x\n",
    "        else:\n",
    "            self.rightbranch = Sequential(\n",
    "                Conv2d(in_channels=in_feats, out_channels=out_feats, kernel_size=1, stride=first_stride),\n",
    "                BatchNorm2d(num_features=out_feats),\n",
    "                \n",
    "            )\n",
    "    \n",
    "    @jaxtyped(typechecked)\n",
    "    def forward(self, x: Float[Tensor, \"b i h w\"]) -> Float[Tensor, \"b o p n\"]:\n",
    "        \"\"\"\n",
    "        Compute the forward pass.\n",
    "\n",
    "        x: shape (batch, in_feats, height, width)\n",
    "\n",
    "        Return: shape (batch, out_feats, height / stride, width / stride)\n",
    "\n",
    "        If no downsampling block is present, the addition should just add the left branch's output to the input.\n",
    "        \"\"\"\n",
    "        x = self.rightbranch(x) + self.leftbranch(x)\n",
    "        x = ReLU()(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "tests.test_residual_block(ResidualBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed all tests when first_stride=1\n",
      "Passed all tests when first_stride>1\n",
      "Passed all tests when n_blocks>2\n",
      "All tests in `test_block_group` passed!\n"
     ]
    }
   ],
   "source": [
    "class BlockGroup(nn.Module):\n",
    "    def __init__(self, n_blocks: int, in_feats: int, out_feats: int, first_stride=1):\n",
    "        \"\"\"An n_blocks-long sequence of ResidualBlock where only the first block uses the provided stride.\"\"\"\n",
    "        super().__init__()\n",
    "        blocks = [ResidualBlock(in_feats=in_feats, out_feats=out_feats, first_stride=first_stride)] + \\\n",
    "        [ResidualBlock(in_feats=out_feats, out_feats=out_feats, first_stride=1) for x in range(n_blocks-1)]\n",
    "\n",
    "        self.module = Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute the forward pass.\n",
    "\n",
    "        x: shape (batch, in_feats, height, width)\n",
    "\n",
    "        Return: shape (batch, out_feats, height / first_stride, width / first_stride)\n",
    "        \"\"\"\n",
    "        return self.module(x)\n",
    "\n",
    "\n",
    "tests.test_block_group(BlockGroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet34(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_blocks_per_group=[3, 4, 6, 3],\n",
    "        out_features_per_group=[64, 128, 256, 512],\n",
    "        first_strides_per_group=[1, 2, 2, 2],\n",
    "        n_classes=1000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        in_feats0 = 64\n",
    "        self.n_blocks_per_group = n_blocks_per_group\n",
    "        self.out_features_per_group = out_features_per_group\n",
    "        self.first_strides_per_group = first_strides_per_group\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        start = [Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3),\n",
    "            BatchNorm2d(num_features=64),\n",
    "            ReLU(),\n",
    "            MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            BlockGroup(n_blocks=n_blocks_per_group[0], in_feats=64, out_feats=out_features_per_group[0], first_stride=first_strides_per_group[0]),]\n",
    "\n",
    "        blocks = [BlockGroup(n_blocks=n_blocks_per_group[i], in_feats=out_features_per_group[i-1], \n",
    "                             out_feats=out_features_per_group[i], first_stride=first_strides_per_group[i]) for i in range(1, 4)]\n",
    "        end = [AveragePool(), Linear(in_features=out_features_per_group[-1], out_features=1000)]\n",
    "\n",
    "        self.sequence = Sequential(\n",
    "            *(start + blocks + end)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        x: shape (batch, channels, height, width)\n",
    "        Return: shape (batch, n_classes)\n",
    "        \"\"\"\n",
    "        self.sequence(x)\n",
    "\n",
    "my_resnet = ResNet34()\n",
    "\n",
    "# (1) Test via helper function `print_param_count`\n",
    "target_resnet = models.resnet34()  # without supplying a `weights` argument, we just initialize with random weights\n",
    "utils.print_param_count(my_resnet, target_resnet)\n",
    "\n",
    "# (2) Test via `torchinfo.summary`\n",
    "print(\"My model:\", torchinfo.summary(my_resnet, input_size=(1, 3, 64, 64)), sep=\"\\n\")\n",
    "print(\"\\nReference model:\", torchinfo.summary(target_resnet, input_size=(1, 3, 64, 64), depth=2), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResNet                                   [1, 1000]                 --\n",
      "Conv2d: 1-1                            [1, 64, 32, 32]           9,408\n",
      "BatchNorm2d: 1-2                       [1, 64, 32, 32]           128\n",
      "ReLU: 1-3                              [1, 64, 32, 32]           --\n",
      "MaxPool2d: 1-4                         [1, 64, 16, 16]           --\n",
      "Sequential: 1-5                        [1, 64, 16, 16]           --\n",
      "    BasicBlock: 2-1                   [1, 64, 16, 16]           --\n",
      "        Conv2d: 3-1                  [1, 64, 16, 16]           36,864\n",
      "        BatchNorm2d: 3-2             [1, 64, 16, 16]           128\n",
      "        ReLU: 3-3                    [1, 64, 16, 16]           --\n",
      "        Conv2d: 3-4                  [1, 64, 16, 16]           36,864\n",
      "        BatchNorm2d: 3-5             [1, 64, 16, 16]           128\n",
      "        ReLU: 3-6                    [1, 64, 16, 16]           --\n",
      "    BasicBlock: 2-2                   [1, 64, 16, 16]           --\n",
      "        Conv2d: 3-7                  [1, 64, 16, 16]           36,864\n",
      "        BatchNorm2d: 3-8             [1, 64, 16, 16]           128\n",
      "        ReLU: 3-9                    [1, 64, 16, 16]           --\n",
      "        Conv2d: 3-10                 [1, 64, 16, 16]           36,864\n",
      "        BatchNorm2d: 3-11            [1, 64, 16, 16]           128\n",
      "        ReLU: 3-12                   [1, 64, 16, 16]           --\n",
      "    BasicBlock: 2-3                   [1, 64, 16, 16]           --\n",
      "        Conv2d: 3-13                 [1, 64, 16, 16]           36,864\n",
      "        BatchNorm2d: 3-14            [1, 64, 16, 16]           128\n",
      "        ReLU: 3-15                   [1, 64, 16, 16]           --\n",
      "        Conv2d: 3-16                 [1, 64, 16, 16]           36,864\n",
      "        BatchNorm2d: 3-17            [1, 64, 16, 16]           128\n",
      "        ReLU: 3-18                   [1, 64, 16, 16]           --\n",
      "Sequential: 1-6                        [1, 128, 8, 8]            --\n",
      "    BasicBlock: 2-4                   [1, 128, 8, 8]            --\n",
      "        Conv2d: 3-19                 [1, 128, 8, 8]            73,728\n",
      "        BatchNorm2d: 3-20            [1, 128, 8, 8]            256\n",
      "        ReLU: 3-21                   [1, 128, 8, 8]            --\n",
      "        Conv2d: 3-22                 [1, 128, 8, 8]            147,456\n",
      "        BatchNorm2d: 3-23            [1, 128, 8, 8]            256\n",
      "        Sequential: 3-24             [1, 128, 8, 8]            8,448\n",
      "        ReLU: 3-25                   [1, 128, 8, 8]            --\n",
      "    BasicBlock: 2-5                   [1, 128, 8, 8]            --\n",
      "        Conv2d: 3-26                 [1, 128, 8, 8]            147,456\n",
      "        BatchNorm2d: 3-27            [1, 128, 8, 8]            256\n",
      "        ReLU: 3-28                   [1, 128, 8, 8]            --\n",
      "        Conv2d: 3-29                 [1, 128, 8, 8]            147,456\n",
      "        BatchNorm2d: 3-30            [1, 128, 8, 8]            256\n",
      "        ReLU: 3-31                   [1, 128, 8, 8]            --\n",
      "    BasicBlock: 2-6                   [1, 128, 8, 8]            --\n",
      "        Conv2d: 3-32                 [1, 128, 8, 8]            147,456\n",
      "        BatchNorm2d: 3-33            [1, 128, 8, 8]            256\n",
      "        ReLU: 3-34                   [1, 128, 8, 8]            --\n",
      "        Conv2d: 3-35                 [1, 128, 8, 8]            147,456\n",
      "        BatchNorm2d: 3-36            [1, 128, 8, 8]            256\n",
      "        ReLU: 3-37                   [1, 128, 8, 8]            --\n",
      "    BasicBlock: 2-7                   [1, 128, 8, 8]            --\n",
      "        Conv2d: 3-38                 [1, 128, 8, 8]            147,456\n",
      "        BatchNorm2d: 3-39            [1, 128, 8, 8]            256\n",
      "        ReLU: 3-40                   [1, 128, 8, 8]            --\n",
      "        Conv2d: 3-41                 [1, 128, 8, 8]            147,456\n",
      "        BatchNorm2d: 3-42            [1, 128, 8, 8]            256\n",
      "        ReLU: 3-43                   [1, 128, 8, 8]            --\n",
      "Sequential: 1-7                        [1, 256, 4, 4]            --\n",
      "    BasicBlock: 2-8                   [1, 256, 4, 4]            --\n",
      "        Conv2d: 3-44                 [1, 256, 4, 4]            294,912\n",
      "        BatchNorm2d: 3-45            [1, 256, 4, 4]            512\n",
      "        ReLU: 3-46                   [1, 256, 4, 4]            --\n",
      "        Conv2d: 3-47                 [1, 256, 4, 4]            589,824\n",
      "        BatchNorm2d: 3-48            [1, 256, 4, 4]            512\n",
      "        Sequential: 3-49             [1, 256, 4, 4]            33,280\n",
      "        ReLU: 3-50                   [1, 256, 4, 4]            --\n",
      "    BasicBlock: 2-9                   [1, 256, 4, 4]            --\n",
      "        Conv2d: 3-51                 [1, 256, 4, 4]            589,824\n",
      "        BatchNorm2d: 3-52            [1, 256, 4, 4]            512\n",
      "        ReLU: 3-53                   [1, 256, 4, 4]            --\n",
      "        Conv2d: 3-54                 [1, 256, 4, 4]            589,824\n",
      "        BatchNorm2d: 3-55            [1, 256, 4, 4]            512\n",
      "        ReLU: 3-56                   [1, 256, 4, 4]            --\n",
      "    BasicBlock: 2-10                  [1, 256, 4, 4]            --\n",
      "        Conv2d: 3-57                 [1, 256, 4, 4]            589,824\n",
      "        BatchNorm2d: 3-58            [1, 256, 4, 4]            512\n",
      "        ReLU: 3-59                   [1, 256, 4, 4]            --\n",
      "        Conv2d: 3-60                 [1, 256, 4, 4]            589,824\n",
      "        BatchNorm2d: 3-61            [1, 256, 4, 4]            512\n",
      "        ReLU: 3-62                   [1, 256, 4, 4]            --\n",
      "    BasicBlock: 2-11                  [1, 256, 4, 4]            --\n",
      "        Conv2d: 3-63                 [1, 256, 4, 4]            589,824\n",
      "        BatchNorm2d: 3-64            [1, 256, 4, 4]            512\n",
      "        ReLU: 3-65                   [1, 256, 4, 4]            --\n",
      "        Conv2d: 3-66                 [1, 256, 4, 4]            589,824\n",
      "        BatchNorm2d: 3-67            [1, 256, 4, 4]            512\n",
      "        ReLU: 3-68                   [1, 256, 4, 4]            --\n",
      "    BasicBlock: 2-12                  [1, 256, 4, 4]            --\n",
      "        Conv2d: 3-69                 [1, 256, 4, 4]            589,824\n",
      "        BatchNorm2d: 3-70            [1, 256, 4, 4]            512\n",
      "        ReLU: 3-71                   [1, 256, 4, 4]            --\n",
      "        Conv2d: 3-72                 [1, 256, 4, 4]            589,824\n",
      "        BatchNorm2d: 3-73            [1, 256, 4, 4]            512\n",
      "        ReLU: 3-74                   [1, 256, 4, 4]            --\n",
      "    BasicBlock: 2-13                  [1, 256, 4, 4]            --\n",
      "        Conv2d: 3-75                 [1, 256, 4, 4]            589,824\n",
      "        BatchNorm2d: 3-76            [1, 256, 4, 4]            512\n",
      "        ReLU: 3-77                   [1, 256, 4, 4]            --\n",
      "        Conv2d: 3-78                 [1, 256, 4, 4]            589,824\n",
      "        BatchNorm2d: 3-79            [1, 256, 4, 4]            512\n",
      "        ReLU: 3-80                   [1, 256, 4, 4]            --\n",
      "Sequential: 1-8                        [1, 512, 2, 2]            --\n",
      "    BasicBlock: 2-14                  [1, 512, 2, 2]            --\n",
      "        Conv2d: 3-81                 [1, 512, 2, 2]            1,179,648\n",
      "        BatchNorm2d: 3-82            [1, 512, 2, 2]            1,024\n",
      "        ReLU: 3-83                   [1, 512, 2, 2]            --\n",
      "        Conv2d: 3-84                 [1, 512, 2, 2]            2,359,296\n",
      "        BatchNorm2d: 3-85            [1, 512, 2, 2]            1,024\n",
      "        Sequential: 3-86             [1, 512, 2, 2]            132,096\n",
      "        ReLU: 3-87                   [1, 512, 2, 2]            --\n",
      "    BasicBlock: 2-15                  [1, 512, 2, 2]            --\n",
      "        Conv2d: 3-88                 [1, 512, 2, 2]            2,359,296\n",
      "        BatchNorm2d: 3-89            [1, 512, 2, 2]            1,024\n",
      "        ReLU: 3-90                   [1, 512, 2, 2]            --\n",
      "        Conv2d: 3-91                 [1, 512, 2, 2]            2,359,296\n",
      "        BatchNorm2d: 3-92            [1, 512, 2, 2]            1,024\n",
      "        ReLU: 3-93                   [1, 512, 2, 2]            --\n",
      "    BasicBlock: 2-16                  [1, 512, 2, 2]            --\n",
      "        Conv2d: 3-94                 [1, 512, 2, 2]            2,359,296\n",
      "        BatchNorm2d: 3-95            [1, 512, 2, 2]            1,024\n",
      "        ReLU: 3-96                   [1, 512, 2, 2]            --\n",
      "        Conv2d: 3-97                 [1, 512, 2, 2]            2,359,296\n",
      "        BatchNorm2d: 3-98            [1, 512, 2, 2]            1,024\n",
      "        ReLU: 3-99                   [1, 512, 2, 2]            --\n",
      "AdaptiveAvgPool2d: 1-9                 [1, 512, 1, 1]            --\n",
      "Linear: 1-10                           [1, 1000]                 513,000\n",
      "==========================================================================================\n",
      "Total params: 21,797,672\n",
      "Trainable params: 21,797,672\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 299.57\n",
      "==========================================================================================\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 4.89\n",
      "Params size (MB): 87.19\n",
      "Estimated Total Size (MB): 92.13\n",
      "==========================================================================================\n",
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "ResNet34                                           --                        --\n",
      "Sequential: 1-1                                  [1, 1000]                 --\n",
      "    Conv2d: 2-1                                 [1, 64, 32, 32]           9,408\n",
      "    BatchNorm2d: 2-2                            [1, 64, 32, 32]           128\n",
      "    ReLU: 2-3                                   [1, 64, 32, 32]           --\n",
      "    MaxPool2d: 2-4                              [1, 64, 32, 32]           --\n",
      "    BlockGroup: 2-5                             [1, 64, 32, 32]           --\n",
      "        Sequential: 3-1                        [1, 64, 32, 32]           221,952\n",
      "    BlockGroup: 2-6                             [1, 128, 16, 16]          --\n",
      "        Sequential: 3-2                        [1, 128, 16, 16]          1,116,416\n",
      "    BlockGroup: 2-7                             [1, 256, 8, 8]            --\n",
      "        Sequential: 3-3                        [1, 256, 8, 8]            6,822,400\n",
      "    BlockGroup: 2-8                             [1, 512, 4, 4]            --\n",
      "        Sequential: 3-4                        [1, 512, 4, 4]            13,114,368\n",
      "    AveragePool: 2-9                            [1, 512]                  --\n",
      "    Linear: 2-10                                [1, 1000]                 513,000\n",
      "====================================================================================================\n",
      "Total params: 21,797,672\n",
      "Trainable params: 21,797,672\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 1.17\n",
      "====================================================================================================\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 16.39\n",
      "Params size (MB): 87.19\n",
      "Estimated Total Size (MB): 103.63\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet34()\n",
    "print(torchinfo.summary(resnet, input_size=(1, 3, 64, 64)))\n",
    "print(torchinfo.summary(my_resnet, input_size=(1, 3, 64, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to C:\\Users\\ander/.cache\\torch\\hub\\checkpoints\\resnet34-b627a593.pth\n",
      "100%|| 83.3M/83.3M [00:10<00:00, 8.17MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights copied successfully!\n"
     ]
    }
   ],
   "source": [
    "def copy_weights(my_resnet: ResNet34, pretrained_resnet: models.resnet.ResNet) -> ResNet34:\n",
    "    \"\"\"Copy over the weights of `pretrained_resnet` to your resnet.\"\"\"\n",
    "\n",
    "    # Get the state dictionaries for each model, check they have the same number of parameters & buffers\n",
    "    mydict = my_resnet.state_dict()\n",
    "    pretraineddict = pretrained_resnet.state_dict()\n",
    "    assert len(mydict) == len(pretraineddict), \"Mismatching state dictionaries.\"\n",
    "\n",
    "    # Define a dictionary mapping the names of your parameters / buffers to their values in the pretrained model\n",
    "    state_dict_to_load = {\n",
    "        mykey: pretrainedvalue\n",
    "        for (mykey, myvalue), (pretrainedkey, pretrainedvalue) in zip(mydict.items(), pretraineddict.items())\n",
    "    }\n",
    "\n",
    "    # Load in this dictionary to your model\n",
    "    my_resnet.load_state_dict(state_dict_to_load)\n",
    "\n",
    "    return my_resnet\n",
    "\n",
    "\n",
    "pretrained_resnet = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1).to(device)\n",
    "my_resnet = copy_weights(my_resnet, pretrained_resnet).to(device)\n",
    "print(\"Weights copied successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Private\\Interesting Stuff\\Coding\\ARENA\\ARENA_3.0\\chapter0_fundamentals\\exercises\\part2_cnns\\resnet_inputs\n"
     ]
    }
   ],
   "source": [
    "IMAGE_FILENAMES = [\n",
    "    \"chimpanzee.jpg\",\n",
    "    \"golden_retriever.jpg\",\n",
    "    \"platypus.jpg\",\n",
    "    \"frogs.jpg\",\n",
    "    \"fireworks.jpg\",\n",
    "    \"astronaut.jpg\",\n",
    "    \"iguana.jpg\",\n",
    "    \"volcano.jpg\",\n",
    "    \"goofy.jpg\",\n",
    "    \"dragonfly.jpg\",\n",
    "]\n",
    "\n",
    "IMAGE_FOLDER = section_dir / \"resnet_inputs\"\n",
    "\n",
    "print(IMAGE_FOLDER)\n",
    "\n",
    "images = [Image.open(IMAGE_FOLDER / filename) for filename in IMAGE_FILENAMES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "IMAGENET_TRANSFORM = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prepared_images = t.stack([IMAGENET_TRANSFORM(img) for img in images], dim=0).to(device)\n",
    "assert prepared_images.shape == (len(images), 3, IMAGE_SIZE, IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "softmax() received an invalid combination of arguments - got (NoneType, dim=int), but expected one of:\n * (Tensor input, int dim, torch.dtype dtype = None, *, Tensor out = None)\n * (Tensor input, name dim, *, torch.dtype dtype = None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[162], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     imagenet_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(json\u001b[38;5;241m.\u001b[39mload(f)\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Check your predictions match those of the pretrained model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m my_probs, my_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_resnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepared_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m pretrained_probs, pretrained_predictions \u001b[38;5;241m=\u001b[39m predict(pretrained_resnet, prepared_images)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (my_predictions \u001b[38;5;241m==\u001b[39m pretrained_predictions)\u001b[38;5;241m.\u001b[39mall()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[162], line 7\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(model, images)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mReturns the maximum probability and predicted class for each image, as a tensor of floats and ints respectively.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m output \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m----> 7\u001b[0m floats \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mmax(floats, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: softmax() received an invalid combination of arguments - got (NoneType, dim=int), but expected one of:\n * (Tensor input, int dim, torch.dtype dtype = None, *, Tensor out = None)\n * (Tensor input, name dim, *, torch.dtype dtype = None)\n"
     ]
    }
   ],
   "source": [
    "@t.inference_mode()\n",
    "def predict(model, images: Float[Tensor, \"batch rgb h w\"]) -> tuple[Float[Tensor, \"batch\"], Int[Tensor, \"batch\"]]:\n",
    "    \"\"\"\n",
    "    Returns the maximum probability and predicted class for each image, as a tensor of floats and ints respectively.\n",
    "    \"\"\"\n",
    "    output = model(images)\n",
    "    floats = t.softmax(output, dim=-1)\n",
    "    return t.max(floats, dim=-1)\n",
    "\n",
    "\n",
    "with open(section_dir / \"imagenet_labels.json\") as f:\n",
    "    imagenet_labels = list(json.load(f).values())\n",
    "\n",
    "# Check your predictions match those of the pretrained model\n",
    "my_probs, my_predictions = predict(my_resnet, prepared_images)\n",
    "pretrained_probs, pretrained_predictions = predict(pretrained_resnet, prepared_images)\n",
    "assert (my_predictions == pretrained_predictions).all()\n",
    "t.testing.assert_close(my_probs, pretrained_probs, atol=5e-4, rtol=0)  # tolerance of 0.05%\n",
    "print(\"All predictions match!\")\n",
    "\n",
    "# Print out your predictions, next to the corresponding images\n",
    "for i, img in enumerate(images):\n",
    "    table = Table(\"Model\", \"Prediction\", \"Probability\")\n",
    "    table.add_row(\"My ResNet\", imagenet_labels[my_predictions[i]], f\"{my_probs[i]:.3%}\")\n",
    "    table.add_row(\"Reference Model\", imagenet_labels[pretrained_predictions[i]], f\"{pretrained_probs[i]:.3%}\")\n",
    "    rprint(table)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN output from NanModule()\n"
     ]
    }
   ],
   "source": [
    "class NanModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Define a module that always returns NaNs (we will use hooks to identify this error).\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return t.full_like(x, float(\"nan\"))\n",
    "\n",
    "\n",
    "def hook_check_for_nan_output(module: nn.Module, input: tuple[Tensor], output: Tensor) -> None:\n",
    "    \"\"\"\n",
    "    Hook function which detects when the output of a layer is NaN.\n",
    "    \"\"\"\n",
    "    if t.isnan(output).any():\n",
    "        raise ValueError(f\"NaN output from {module}\")\n",
    "\n",
    "\n",
    "def add_hook(module: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Register our hook function in a module.\n",
    "\n",
    "    Use model.apply(add_hook) to recursively apply the hook to model and all submodules.\n",
    "    \"\"\"\n",
    "    module.register_forward_hook(hook_check_for_nan_output)\n",
    "\n",
    "\n",
    "def remove_hooks(module: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Remove all hooks from module.\n",
    "\n",
    "    Use module.apply(remove_hooks) to do this recursively.\n",
    "    \"\"\"\n",
    "    module._backward_hooks.clear()\n",
    "    module._forward_hooks.clear()\n",
    "    module._forward_pre_hooks.clear()\n",
    "\n",
    "\n",
    "# Create our model with a NaN in the middle, and apply a hook function to it which checks for NaNs\n",
    "model = nn.Sequential(nn.Identity(), NanModule(), nn.Identity())\n",
    "model = model.apply(add_hook)\n",
    "\n",
    "# Run the model, and and our hook function should raise an error that gets caught by the try-except\n",
    "try:\n",
    "    input = t.randn(3)\n",
    "    output = model(input)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "# Remove hooks at the end\n",
    "model = model.apply(remove_hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
